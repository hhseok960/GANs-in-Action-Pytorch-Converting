{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1f93734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shdhk\\anaconda3\\envs\\agent\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\shdhk\\anaconda3\\envs\\agent\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6f0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "CHANNELS, IMG_ROWS, IMG_COLS = 3, 128, 128\n",
    "IMG_SHAPE = (CHANNELS, IMG_ROWS, IMG_COLS)\n",
    "Z_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33cfe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_dataset, apple_img_path = [], \"./apple2orange/trainA\"\n",
    "orange_dataset, orange_img_path = [], \"./apple2orange/trainB\"\n",
    "apple_img_list, orange_img_list = os.listdir(apple_img_path), os.listdir(orange_img_path)\n",
    "num_apple, num_orange = len(apple_img_list), len(orange_img_list)\n",
    "\n",
    "for img_name in apple_img_list:\n",
    "    img = cv2.imread(f\"{apple_img_path}/{img_name}\")\n",
    "    img = cv2.resize(img, dsize=(IMG_ROWS, IMG_COLS))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    apple_dataset.append(torch.FloatTensor(img))\n",
    "\n",
    "for img_name in orange_img_list:\n",
    "    img = cv2.imread(f\"{orange_img_path}/{img_name}\")\n",
    "    img = cv2.resize(img, dsize=(IMG_ROWS, IMG_COLS))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    orange_dataset.append(torch.FloatTensor(img))\n",
    "\n",
    "apple_dataset, orange_dataset = torch.cat(apple_dataset, dim=0).to(DEVICE), torch.cat(orange_dataset, dim=0).to(DEVICE)\n",
    "apple_dataset, orange_dataset = (apple_dataset / 127.5) - 1.0, (orange_dataset / 127.5) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fec9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]          32,832\n",
      "    InstanceNorm2d-2           [-1, 64, 32, 32]               0\n",
      "================================================================\n",
      "Total params: 32,832\n",
      "Trainable params: 32,832\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.50\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 0.13\n",
      "Estimated Total Size (MB): 1.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, f_size=4, normalization=True):\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                              kernel_size=f_size, stride=2, padding=1)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        self.normalization = normalization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv(x), negative_slope=0.01)\n",
    "        if self.normalization:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "conv2d = Conv2d(in_channels=32, out_channels=64).to(DEVICE)\n",
    "summary(conv2d, (32, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ffa8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "          Upsample-1          [-1, 256, 16, 16]               0\n",
      "            Conv2d-2          [-1, 128, 16, 16]         524,416\n",
      "    InstanceNorm2d-3          [-1, 128, 16, 16]               0\n",
      "================================================================\n",
      "Total params: 524,416\n",
      "Trainable params: 524,416\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2048.00\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 2.00\n",
      "Estimated Total Size (MB): 2051.00\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shdhk\\anaconda3\\envs\\agent\\lib\\site-packages\\torch\\nn\\modules\\conv.py:442: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ..\\aten\\src\\ATen\\native\\Convolution.cpp:647.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "class DeConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, f_size=4, dropout_rate=0):\n",
    "        super(DeConv2d, self).__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                              kernel_size=f_size, stride=1, padding='same')\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.conv(x))\n",
    "        if self.dropout_rate:\n",
    "            x = self.dropout(x)\n",
    "        x = self.norm(x)\n",
    "        output = torch.cat((x, skip_input), dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "deconv2d = DeConv2d(256, 128).to(DEVICE)\n",
    "summary(deconv2d, [(256, 8, 8), (128, 16, 16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc234922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]           1,568\n",
      "    InstanceNorm2d-2           [-1, 32, 64, 64]               0\n",
      "            Conv2d-3           [-1, 32, 64, 64]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]          32,832\n",
      "    InstanceNorm2d-5           [-1, 64, 32, 32]               0\n",
      "            Conv2d-6           [-1, 64, 32, 32]               0\n",
      "            Conv2d-7          [-1, 128, 16, 16]         131,200\n",
      "    InstanceNorm2d-8          [-1, 128, 16, 16]               0\n",
      "            Conv2d-9          [-1, 128, 16, 16]               0\n",
      "           Conv2d-10            [-1, 256, 8, 8]         524,544\n",
      "   InstanceNorm2d-11            [-1, 256, 8, 8]               0\n",
      "           Conv2d-12            [-1, 256, 8, 8]               0\n",
      "         Upsample-13          [-1, 256, 16, 16]               0\n",
      "           Conv2d-14          [-1, 128, 16, 16]         524,416\n",
      "   InstanceNorm2d-15          [-1, 128, 16, 16]               0\n",
      "         DeConv2d-16          [-1, 256, 16, 16]               0\n",
      "         Upsample-17          [-1, 256, 32, 32]               0\n",
      "           Conv2d-18           [-1, 64, 32, 32]         262,208\n",
      "   InstanceNorm2d-19           [-1, 64, 32, 32]               0\n",
      "         DeConv2d-20          [-1, 128, 32, 32]               0\n",
      "         Upsample-21          [-1, 128, 64, 64]               0\n",
      "           Conv2d-22           [-1, 32, 64, 64]          65,568\n",
      "   InstanceNorm2d-23           [-1, 32, 64, 64]               0\n",
      "         DeConv2d-24           [-1, 64, 64, 64]               0\n",
      "         Upsample-25         [-1, 64, 128, 128]               0\n",
      "           Conv2d-26          [-1, 3, 128, 128]           3,075\n",
      "             Tanh-27          [-1, 3, 128, 128]               0\n",
      "================================================================\n",
      "Total params: 1,545,411\n",
      "Trainable params: 1,545,411\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 27.88\n",
      "Params size (MB): 5.90\n",
      "Estimated Total Size (MB): 33.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_shape, gf=32):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = Conv2d(in_channels=img_shape[0], out_channels=gf).to(DEVICE)\n",
    "        self.conv2 = Conv2d(in_channels=gf, out_channels=gf * 2).to(DEVICE)\n",
    "        self.conv3 = Conv2d(in_channels=gf * 2, out_channels=gf * 4).to(DEVICE)\n",
    "        self.conv4 = Conv2d(in_channels=gf * 4, out_channels=gf * 8).to(DEVICE)\n",
    "        self.deconv1 = DeConv2d(in_channels=gf * 8, out_channels=gf * 4).to(DEVICE)\n",
    "        self.deconv2 = DeConv2d(in_channels=gf * 8, out_channels=gf * 2).to(DEVICE)\n",
    "        self.deconv3 = DeConv2d(in_channels=gf * 4, out_channels=gf).to(DEVICE)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.conv5 = nn.Conv2d(in_channels=gf * 2, out_channels=img_shape[0], kernel_size=4, stride=1, padding=\"same\")\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.conv1(x)\n",
    "        d2 = self.conv2(d1)\n",
    "        d3 = self.conv3(d2)\n",
    "        d4 = self.conv4(d3)\n",
    "        u1 = self.deconv1(d4, d3)\n",
    "        u2 = self.deconv2(u1, d2)\n",
    "        u3 = self.deconv3(u2, d1)\n",
    "        u4 = self.upsample(u3)\n",
    "        output = self.tanh(self.conv5(u4))\n",
    "        return output\n",
    "\n",
    "\n",
    "generator_AB = Generator(img_shape=(3, 128, 128)).to(DEVICE)\n",
    "generator_BA = Generator(img_shape=(3, 128, 128)).to(DEVICE)\n",
    "summary(generator_AB, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21878517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 64, 64]           3,136\n",
      "            Conv2d-2           [-1, 64, 64, 64]               0\n",
      "            Conv2d-3          [-1, 128, 32, 32]         131,200\n",
      "    InstanceNorm2d-4          [-1, 128, 32, 32]               0\n",
      "            Conv2d-5          [-1, 128, 32, 32]               0\n",
      "            Conv2d-6          [-1, 256, 16, 16]         524,544\n",
      "    InstanceNorm2d-7          [-1, 256, 16, 16]               0\n",
      "            Conv2d-8          [-1, 256, 16, 16]               0\n",
      "            Conv2d-9            [-1, 512, 8, 8]       2,097,664\n",
      "   InstanceNorm2d-10            [-1, 512, 8, 8]               0\n",
      "           Conv2d-11            [-1, 512, 8, 8]               0\n",
      "           Conv2d-12              [-1, 1, 8, 8]           8,193\n",
      "================================================================\n",
      "Total params: 2,764,737\n",
      "Trainable params: 2,764,737\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 9.25\n",
      "Params size (MB): 10.55\n",
      "Estimated Total Size (MB): 19.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_shape, df=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = Conv2d(in_channels=img_shape[0], out_channels=df, normalization=False).to(DEVICE)\n",
    "        self.conv2 = Conv2d(in_channels=df, out_channels=df * 2).to(DEVICE)\n",
    "        self.conv3 = Conv2d(in_channels=df * 2, out_channels=df * 4).to(DEVICE)\n",
    "        self.conv4 = Conv2d(in_channels=df * 4, out_channels=df * 8).to(DEVICE)\n",
    "        self.conv5 = nn.Conv2d(in_channels=df * 8, out_channels=1, kernel_size=4, stride=1, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2(self.conv1(x))\n",
    "        x = self.conv4(self.conv3(x))\n",
    "        validity = self.conv5(x)\n",
    "        return validity\n",
    "    \n",
    "\n",
    "discriminator_A = Discriminator(img_shape=(3, 128, 128)).to(DEVICE)\n",
    "discriminator_B = Discriminator(img_shape=(3, 128, 128)).to(DEVICE)\n",
    "summary(discriminator_A, (3, 128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28937a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_dis_A = optim.Adam(discriminator_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optim_dis_B = optim.Adam(discriminator_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optim_gen = optim.Adam(itertools.chain(generator_AB.parameters(), generator_BA.parameters()), \n",
    "                       lr=0.0002, betas=(0.5, 0.999))\n",
    "criterion_cycle_loss = nn.L1Loss()\n",
    "criterion_adversarial_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(gen_AB, gen_BA, iteration, path=\"./Chapter09_image\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    img_A_idx = np.random.randint(low=0, high=apple_dataset.shape[0], size=1)\n",
    "    img_B_idx = np.random.randint(low=0, high=orange_dataset.shape[0], size=1)\n",
    "    img_A, img_B = apple_dataset[img_A_idx], orange_dataset[img_B_idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_AB.eval()\n",
    "        gen_BA.eval()\n",
    "        fake_B, fake_A = gen_AB(img_A), gen_BA(img_B)\n",
    "        reconstr_A, reconstr_B = gen_BA(fake_B), gen_AB(fake_A)\n",
    "        gen_imgs = torch.cat([img_A, fake_B, reconstr_A, img_B, fake_A, reconstr_B], dim=0)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        gen_imgs = gen_imgs.detach().to(torch.device(\"cpu\")).numpy()\n",
    "    titles = [\"Original\", \"Translated\", \"Reconstructed\"]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(6, 4), sharex=True, sharey=True)\n",
    "    cnt = 0\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            axes[i, j].imshow(gen_imgs[cnt].transpose(1, 2, 0))\n",
    "            axes[i, j].set_title(titles[j])\n",
    "            axes[i, j].axis(\"off\")\n",
    "            cnt += 1\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{path}/img_{iteration:03d}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc5bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 [D loss: 0.1560] [G loss: 11.3459]\n",
      "200 [D loss: 0.0818] [G loss: 10.7927]\n",
      "300 [D loss: 0.0673] [G loss: 9.3598]\n",
      "400 [D loss: 0.2565] [G loss: 8.6609]\n",
      "500 [D loss: 0.0510] [G loss: 8.8316]\n",
      "600 [D loss: 0.0355] [G loss: 8.6586]\n",
      "700 [D loss: 0.0308] [G loss: 8.1440]\n",
      "800 [D loss: 0.0905] [G loss: 8.0540]\n",
      "900 [D loss: 0.0307] [G loss: 7.7996]\n",
      "1000 [D loss: 0.0216] [G loss: 8.1018]\n",
      "1100 [D loss: 0.0934] [G loss: 7.3984]\n",
      "1200 [D loss: 0.1553] [G loss: 6.8660]\n",
      "1300 [D loss: 0.0462] [G loss: 7.6937]\n",
      "1400 [D loss: 0.0400] [G loss: 7.1914]\n",
      "1500 [D loss: 0.0415] [G loss: 7.1440]\n",
      "1600 [D loss: 0.0348] [G loss: 7.0255]\n",
      "1700 [D loss: 0.0242] [G loss: 6.8890]\n",
      "1800 [D loss: 0.0618] [G loss: 6.5398]\n",
      "1900 [D loss: 0.0488] [G loss: 6.5681]\n",
      "2000 [D loss: 0.0284] [G loss: 6.7743]\n"
     ]
    }
   ],
   "source": [
    "losses, iteration_checkpoints = [], []\n",
    "iterations = 2000\n",
    "batch_size = 64\n",
    "sample_interval = 100\n",
    "lambda_cycle, lambda_id = 10.0, 9.0\n",
    "\n",
    "valid = torch.ones(batch_size, 1, 8, 8).to(DEVICE)\n",
    "fake = torch.zeros(batch_size, 1, 8, 8).to(DEVICE)\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    idx_apple = np.random.randint(low=0, high=apple_dataset.shape[0], size=batch_size)\n",
    "    idx_orange = np.random.randint(low=0, high=orange_dataset.shape[0], size=batch_size)\n",
    "    imgs_A, imgs_B = apple_dataset[idx_apple], orange_dataset[idx_orange]\n",
    "\n",
    "    fake_A, fake_B = generator_BA(imgs_B).detach(), generator_AB(imgs_A).detach()\n",
    "\n",
    "    optim_dis_A.zero_grad()\n",
    "    dA_pred_real, dA_pred_fake = discriminator_A(imgs_A), discriminator_A(fake_A)\n",
    "    dA_loss_real = criterion_adversarial_loss(dA_pred_real, valid)\n",
    "    dA_loss_fake = criterion_adversarial_loss(dA_pred_fake, fake)\n",
    "    dA_loss = (dA_loss_real + dA_loss_fake) * 0.5\n",
    "    dA_loss.backward()\n",
    "    optim_dis_A.step()\n",
    "\n",
    "    optim_dis_B.zero_grad()\n",
    "    dB_pred_real, dB_pred_fake = discriminator_B(imgs_B), discriminator_B(fake_B)\n",
    "    dB_loss_real = criterion_adversarial_loss(dB_pred_real, valid)\n",
    "    dB_loss_fake = criterion_adversarial_loss(dB_pred_fake, fake)\n",
    "    dB_loss = (dB_loss_real + dB_loss_fake) * 0.5\n",
    "    dB_loss.backward()\n",
    "    optim_dis_B.step()\n",
    "\n",
    "    d_loss = dA_loss + dB_loss\n",
    "    \n",
    "    optim_gen.zero_grad()\n",
    "    fake_B = generator_AB(imgs_A)\n",
    "    fake_A = generator_BA(imgs_B)\n",
    "    reconstr_A = generator_BA(fake_B)\n",
    "    reconstr_B = generator_AB(fake_A)\n",
    "    imgs_A_identical = generator_BA(imgs_A)\n",
    "    imgs_B_identical = generator_AB(imgs_B)\n",
    "\n",
    "    valid_A = discriminator_A(fake_A)\n",
    "    valid_B = discriminator_B(fake_B)\n",
    "\n",
    "    loss_A_reconstr = criterion_cycle_loss(reconstr_A, imgs_A)\n",
    "    loss_B_reconstr = criterion_cycle_loss(reconstr_B, imgs_B)\n",
    "    \n",
    "    loss_A_adversarial = criterion_adversarial_loss(valid_A, valid)\n",
    "    loss_B_adversarial = criterion_adversarial_loss(valid_B, valid)\n",
    "    \n",
    "    loss_A_identical = criterion_cycle_loss(imgs_A_identical, imgs_A)\n",
    "    loss_B_identical = criterion_cycle_loss(imgs_B_identical, imgs_B)\n",
    "    \n",
    "    loss_A_total = loss_A_adversarial + lambda_cycle * loss_A_reconstr + lambda_id * loss_A_identical\n",
    "    loss_B_total = loss_B_adversarial + lambda_cycle * loss_B_reconstr + lambda_id * loss_B_identical\n",
    "    \n",
    "    g_loss = loss_A_total + loss_B_total\n",
    "    g_loss.backward()\n",
    "    optim_gen.step()\n",
    "\n",
    "    if (iteration + 1) % sample_interval == 0:\n",
    "        losses.append([d_loss.item(), g_loss.item()])\n",
    "        iteration_checkpoints.append(iteration + 1)\n",
    "        print(f\"{iteration + 1} [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "        sample_images(generator_AB, generator_BA, iteration=iteration + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78142724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
